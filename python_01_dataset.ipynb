{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import csv\n",
    "from pydriller import RepositoryMining\n",
    "from radon.raw import analyze\n",
    "from radon.metrics import h_visit\n",
    "from radon.metrics import h_visit_ast\n",
    "from radon.complexity import sorted_results\n",
    "from pydriller.git_repository import GitRepository\n",
    "\n",
    "fields = ['CommitID','filename','complexity','token_count','loc','lloc','sloc','comments',\n",
    "          'multi','blank','code_comment','h1','h2','N1','N2','vocabulary','length',\n",
    "          'calculated_length','volume', 'difficulty','effort','time','bugs']\n",
    "\n",
    "with open('python_dataset.csv', 'w') as csvFile:\n",
    "    writer = csv.DictWriter(csvFile, fieldnames = fields)\n",
    "    writer.writeheader()\n",
    "    for commit in RepositoryMining(#path_to_repo#,\n",
    "                                   only_modifications_with_file_types=['.py'] ).traverse_commits():\n",
    "        for modification in commit.modifications:\n",
    "            filename = modification.filename\n",
    "            hash_val = commit.hash\n",
    "            token_count = modification.token_count \n",
    "            complexity = modification.complexity\n",
    "            if filename.endswith(\".py\"):\n",
    "                #Calculate static code metrics\n",
    "                for r, d, f in os.walk(repo_updated):\n",
    "                    for file in f:\n",
    "                        if filename in file:\n",
    "                            file_path = os.path.join(r, file)\n",
    "                            with open(file_path) as f:\n",
    "                                content = f.read()\n",
    "                                file_analyze = analyze(content)\n",
    "                                code_n_comment = file_analyze.loc+file_analyze.comments\n",
    "                                file_ast = h_visit(content)\n",
    "                                data = [{'CommitID':(hash_val), \n",
    "                                         'filename':(file_path),\n",
    "                                         'complexity':(complexity),\n",
    "                                         'token_count':(token_count),\n",
    "                                         'loc':(file_analyze.loc),\n",
    "                                         'lloc':(file_analyze.lloc),\n",
    "                                         'sloc':(file_analyze.sloc),\n",
    "                                         'comments':(file_analyze.comments),\n",
    "                                         'multi':(file_analyze.multi),\n",
    "                                         'blank':(file_analyze.blank),\n",
    "                                         'code_comment':(code_n_comment),\n",
    "                                         'h1':(file_ast.total.h1),\n",
    "                                         'h2':(file_ast.total.h2),\n",
    "                                         'N1':(file_ast.total.N1),\n",
    "                                         'N2':(file_ast.total.N2),\n",
    "                                         'vocabulary':(file_ast.total.vocabulary),\n",
    "                                         'length':(file_ast.total.length),\n",
    "                                         'calculated_length':(file_ast.total.calculated_length),\n",
    "                                         'volume':(file_ast.total.volume),                     \n",
    "                                         'difficulty':(file_ast.total.difficulty),\n",
    "                                         'effort':(file_ast.total.effort),\n",
    "                                         'time':(file_ast.total.time),\n",
    "                                         'bugs':(file_ast.total.bugs),}]\n",
    "                                writer.writerows(data)  \n",
    "print(\"writing completed\")\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydriller.git_repository import GitRepository\n",
    "from pydriller import RepositoryMining\n",
    "gr = GitRepository(#path_to_repo#)\n",
    "\n",
    "buggy_list = [] \n",
    "count = 0\n",
    "for commit in RepositoryMining(#path_to_repo#,\n",
    "                                   only_modifications_with_file_types=['.py'] ).traverse_commits():\n",
    "    if \"fix\" in commit.msg:\n",
    "        commit1 = gr.get_commit(commit.hash)\n",
    "        buggy_commits = gr.get_commits_last_modified_lines(commit1)\n",
    "        for x in buggy_commits:\n",
    "            buggy_list.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify the buggy commits in main file\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"python_dataset.csv\")\n",
    "df.loc[df['CommitID'].isin(buggy_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe \n",
    "df = pd.DataFrame(df, columns = ['CommitID','filename','complexity','token_count','loc','lloc','sloc','comments',\n",
    "          'multi','blank','code_comment','h1','h2','N1','N2','vocabulary','length',\n",
    "          'calculated_length','volume', 'difficulty','effort','time','bugs']) \n",
    "\n",
    "# Label datapoints\n",
    "result = [] \n",
    "for value in df[\"CommitID\"]:\n",
    "    if value in buggy_list:\n",
    "        result.append(True)\n",
    "    else:\n",
    "        result.append(False)\n",
    "    \n",
    "df[\"defect\"] = result \n",
    "#writing release filter labelled data to CSV\n",
    "df.to_csv('python_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch main dataset created and add headers\n",
    "filepath = 'python_dataset.csv'\n",
    "column = ['complexity','token_count','loc','lloc','sloc','comments',\n",
    "          'multi','blank','code_comment','h1','h2','N1','N2','vocabulary','length',\n",
    "          'calculated_length','volume', 'difficulty','effort','time','bugs','defect']\n",
    "df1 = pd.read_csv(filepath, usecols= column, index_col=False)\n",
    "#remove rows where LOC = 0, and complexity = 0\n",
    "col = ['loc','complexity']\n",
    "df1 = df1.replace(0, pd.np.nan).dropna(axis=0,how='any',subset=col).fillna(0).astype(int)\n",
    "\n",
    "#Write labelled data to CSV \n",
    "df1.to_csv('python_01.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
