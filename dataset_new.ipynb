{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import csv\n",
    "# from pydriller import RepositoryMining\n",
    "# from radon.raw import analyze\n",
    "# from radon.metrics import h_visit\n",
    "# from radon.metrics import h_visit_ast\n",
    "# from radon.complexity import sorted_results\n",
    "# from pydriller.git_repository import GitRepository\n",
    "\n",
    "# # directory = '/Users/sanika/Downloads/Master Thesis/Implementation/test_loop_files'\n",
    "\n",
    "# fields = ['loc','lloc','sloc','comments','multi','blank','code_comment','h1',\n",
    "#          'h2','N1','N2','vocabulary','length','calculated_length','volume',                     \n",
    "#          'difficulty','effort','time','bugs']\n",
    "\n",
    "\n",
    "# repo = GitRepository(\"/Users/sanika/Downloads/Master Thesis/repo/pytest\")\n",
    "# files = repo.files()\n",
    "# count = 0\n",
    "\n",
    "# with open('extracted_data_new.csv', 'w') as csvFile:\n",
    "#     writer = csv.DictWriter(csvFile, fieldnames = fields)\n",
    "#     writer.writeheader()\n",
    "#     for filename in files:\n",
    "#         if filename.endswith(\".py\"):\n",
    "#             with open(fil ename) as f:\n",
    "#                 content = f.read()\n",
    "#                 file_analyze = analyze(content) #raw metrics\n",
    "#                 code_n_comment = file_analyze.loc+file_analyze.comments\n",
    "#                 file_ast = h_visit(content) #Compile the code into an AST tree\n",
    "#                 count = count + 1\n",
    "#                 data = [{\n",
    "#                      'loc':(file_analyze.loc),\n",
    "#                      'lloc':(file_analyze.lloc),\n",
    "#                      'sloc':(file_analyze.sloc),\n",
    "#                      'comments':(file_analyze.comments),\n",
    "#                      'multi':(file_analyze.multi),\n",
    "#                      'blank':(file_analyze.blank),\n",
    "#                      'code_comment':(code_n_comment),\n",
    "#                      'h1':(file_ast.total.h1),\n",
    "#                      'h2':(file_ast.total.h2),\n",
    "#                      'N1':(file_ast.total.N1),\n",
    "#                      'N2':(file_ast.total.N2),\n",
    "#                      'vocabulary':(file_ast.total.vocabulary),\n",
    "#                      'length':(file_ast.total.length),\n",
    "#                      'calculated_length':(file_ast.total.calculated_length),\n",
    "#                      'volume':(file_ast.total.volume),                     \n",
    "#                      'difficulty':(file_ast.total.difficulty),\n",
    "#                      'effort':(file_ast.total.effort),\n",
    "#                      'time':(file_ast.total.time),\n",
    "#                      'bugs':(file_ast.total.bugs),                     \n",
    "#                     }]\n",
    "# #                 count = count + 1\n",
    "# #                 print(count, filename)\n",
    "#                 writer.writerows(data)\n",
    "\n",
    "# print(\"writing completed\")\n",
    "# csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import csv\n",
    "# from pydriller import RepositoryMining\n",
    "# from radon.raw import analyze\n",
    "# from radon.metrics import h_visit\n",
    "# from radon.metrics import h_visit_ast\n",
    "# from radon.complexity import sorted_results\n",
    "# from pydriller.git_repository import GitRepository\n",
    "\n",
    "# fields = ['CommitID','filename','loc','lloc','sloc','comments','multi','blank','code_comment','h1',\n",
    "#          'h2','N1','N2','vocabulary','length','calculated_length','volume',                     \n",
    "#          'difficulty','effort','time','bugs']\n",
    "# releases = ['4.6.6','4.6.7','4.6.8','4.6.9','5.3.0','5.3.1','5.3.2','5.3.3','5.3.4','5.3.5']\n",
    "# repo = \"/Users/sanika/Downloads/Master Thesis/repo_updated/pytest\"\n",
    "\n",
    "# # count = 0\n",
    "# i = 0\n",
    "# length = len(releases)-1\n",
    "# while i < length:\n",
    "#     for commit in RepositoryMining(repo,\n",
    "#                                    only_modifications_with_file_types=['.py'],\n",
    "#                                    from_tag=releases[i],\n",
    "#                                    to_tag=releases[i+1]).traverse_commits():\n",
    "#         for modification in commit.modifications:\n",
    "# #             date = commit.committer_date\n",
    "#             mod_file = modification.filename\n",
    "#             hash_val = commit.hash\n",
    "#             i += 1\n",
    "#             with open('extracted_data_000.csv', 'w') as csvFile:\n",
    "#                 writer = csv.DictWriter(csvFile, fieldnames = fields)\n",
    "#                 writer.writeheader()\n",
    "#                 for filename in mod_file: #files:\n",
    "#                     if filename.endswith(\".py\"):\n",
    "#                         print(filename)\n",
    "#                         with open(filename) as f:\n",
    "#                             content = f.read()\n",
    "#                             file_analyze = analyze(content) #raw metrics\n",
    "#                             code_n_comment = file_analyze.loc+file_analyze.comments\n",
    "#                             file_ast = h_visit(content) #Compile the code into an AST tree\n",
    "#                             count = count + 1\n",
    "#                             data = [{'CommitID':(hash_val),\n",
    "#                                      'filename':(mod_file),\n",
    "#                                      'loc':(file_analyze.loc),\n",
    "#                                      'lloc':(file_analyze.lloc),\n",
    "#                                      'sloc':(file_analyze.sloc),\n",
    "#                                      'comments':(file_analyze.comments),\n",
    "#                                      'multi':(file_analyze.multi),\n",
    "#                                      'blank':(file_analyze.blank),\n",
    "#                                      'code_comment':(code_n_comment),\n",
    "#                                      'h1':(file_ast.total.h1),\n",
    "#                                      'h2':(file_ast.total.h2),\n",
    "#                                      'N1':(file_ast.total.N1),\n",
    "#                                      'N2':(file_ast.total.N2),\n",
    "#                                      'vocabulary':(file_ast.total.vocabulary),\n",
    "#                                      'length':(file_ast.total.length),\n",
    "#                                      'calculated_length':(file_ast.total.calculated_length),\n",
    "#                                      'volume':(file_ast.total.volume),                     \n",
    "#                                      'difficulty':(file_ast.total.difficulty),\n",
    "#                                      'effort':(file_ast.total.effort),\n",
    "#                                      'time':(file_ast.total.time),\n",
    "#                                      'bugs':(file_ast.total.bugs),}]\n",
    "#                             writer.writerows(data)\n",
    "\n",
    "# print(\"writing completed\")\n",
    "# csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-cf55624458c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m                         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                             \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                             \u001b[0mfile_analyze\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                             \u001b[0mcode_n_comment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_analyze\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile_analyze\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                             \u001b[0mfile_ast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_visit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/radon/raw.py\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;31m# Get a syntactically complete set of tokens that spans a set of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;31m# lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparsed_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_all_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mSyntaxError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SyntaxError at line: {0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlineno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/radon/raw.py\u001b[0m in \u001b[0;36m_get_all_tokens\u001b[0;34m(line, lines)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTokenError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# A multi-line string or statement has been encountered:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/radon/raw.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(code)\u001b[0m\n\u001b[1;32m     43\u001b[0m     '''\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# tokenize.generate_tokens is an undocumented function accepting text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/tokenize.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(readline, encoding)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m             \u001b[0mpseudomatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPseudoToken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpseudomatch\u001b[0m\u001b[0;34m:\u001b[0m                                \u001b[0;31m# scan for tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m                 \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpseudomatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from pydriller import RepositoryMining\n",
    "from radon.raw import analyze\n",
    "from radon.metrics import h_visit\n",
    "from radon.metrics import h_visit_ast\n",
    "from radon.complexity import sorted_results\n",
    "from pydriller.git_repository import GitRepository\n",
    "\n",
    "fields = ['CommitID','filename','change_type','lines_added','lines_removed','token_count','loc','lloc','sloc','comments','multi','blank','code_comment','h1',\n",
    "         'h2','N1','N2','vocabulary','length','calculated_length','volume',                     \n",
    "         'difficulty','effort','time','bugs']\n",
    "releases = ['4.6.6','4.6.7','4.6.8','4.6.9','5.3.0','5.3.1','5.3.2','5.3.3','5.3.4','5.3.5']\n",
    "repo_updated = \"/Users/sanika/Downloads/Master Thesis/repo_updated/pytest\"\n",
    "    \n",
    "gitrepo = GitRepository(\"/Users/sanika/Downloads/Master Thesis/repo_updated/pytest\")\n",
    "gitfiles = gitrepo.files()\n",
    "\n",
    "count = 0\n",
    "i = 0\n",
    "length = len(releases)-1\n",
    "\n",
    "with open('extracted_data_000.csv', 'w') as csvFile:\n",
    "    writer = csv.DictWriter(csvFile, fieldnames = fields)\n",
    "    writer.writeheader()\n",
    "    while i < length:\n",
    "        for commit in RepositoryMining(repo_updated,\n",
    "                                   only_modifications_with_file_types=['.py'],\n",
    "                                   from_tag=releases[i],\n",
    "                                   to_tag=releases[i+1]).traverse_commits():\n",
    "            for modification in commit.modifications:\n",
    "                filename = modification.filename\n",
    "                for filename in gitfiles:\n",
    "                    if filename.endswith(\".py\"):\n",
    "                        with open(filename) as f:\n",
    "                            content = f.read()\n",
    "                            file_analyze = analyze(content)\n",
    "                            code_n_comment = file_analyze.loc+file_analyze.comments\n",
    "                            file_ast = h_visit(content)\n",
    "                            data = [{'CommitID':(hash_val),\n",
    "                                     'filename':(modification.filename),\n",
    "                                     'token_count':(modification.token_count),\n",
    "                                     'change_type':(modification.change_type),\n",
    "                                     'lines_added':(modification.added),\n",
    "                                     'lines_removed':(modification.removed),\n",
    "                                     'loc':(file_analyze.loc),\n",
    "                                     'lloc':(file_analyze.lloc),\n",
    "                                     'sloc':(file_analyze.sloc),\n",
    "                                     'comments':(file_analyze.comments),\n",
    "                                     'multi':(file_analyze.multi),\n",
    "                                     'blank':(file_analyze.blank),\n",
    "                                     'code_comment':(code_n_comment),\n",
    "                                     'h1':(file_ast.total.h1),\n",
    "                                     'h2':(file_ast.total.h2),\n",
    "                                     'N1':(file_ast.total.N1),\n",
    "                                     'N2':(file_ast.total.N2),\n",
    "                                     'vocabulary':(file_ast.total.vocabulary),\n",
    "                                     'length':(file_ast.total.length),\n",
    "                                     'calculated_length':(file_ast.total.calculated_length),\n",
    "                                     'volume':(file_ast.total.volume),                     \n",
    "                                     'difficulty':(file_ast.total.difficulty),\n",
    "                                     'effort':(file_ast.total.effort),\n",
    "                                     'time':(file_ast.total.time),\n",
    "                                     'bugs':(file_ast.total.bugs),}]\n",
    "                            writer.writerows(data)\n",
    "                            i += 1\n",
    "print(\"writing completed\")\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
