{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required linraries\n",
    "import os\n",
    "from pydriller import RepositoryMining\n",
    "from radon.raw import analyze\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'python_dataset.csv'\n",
    "\n",
    "df_main = pd.read_csv(filepath, index_col=None, usecols = ['CommitID','filename','complexity','token_count',\n",
    "                                                           'loc','lloc','sloc','comments',\n",
    "                                                           'multi','blank','code_comment','h1','h2','N1','N2',\n",
    "                                                           'vocabulary','length','calculated_length','volume', \n",
    "                                                           'difficulty','effort','time','bugs','defect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define lists to append values to\n",
    "hash_val =[]\n",
    "change_type = []\n",
    "removed = []\n",
    "added = []\n",
    "\n",
    "for commit in RepositoryMining(#path_to_repo#,\n",
    "                                   only_modifications_with_file_types=['.py'] ).traverse_commits():\n",
    "        for modification in commit.modifications:\n",
    "            filename = modification.filename\n",
    "            if filename.endswith(\".py\"):\n",
    "                for r, d, f in os.walk(repo_updated):\n",
    "                    for file in f:\n",
    "                        if filename in file:\n",
    "                            file_path = os.path.join(r, file)\n",
    "                            with open(file_path) as f:\n",
    "                                content = f.read()\n",
    "                                file_analyze = analyze(content)\n",
    "                                code_n_comment = file_analyze.loc+file_analyze.comments\n",
    "                                #Append code churn attributes to the lists\n",
    "                                hash_val.append(commit.hash)\n",
    "                                change_type.append(modification.change_type)\n",
    "                                added.append(modification.added)\n",
    "                                removed.append(modification.removed)\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add new columns to the main dataset\n",
    "df_new = pd.DataFrame(columns=['hash_val','change_type','added','removed'])\n",
    "\n",
    "#write labelled data to CSV with new features\n",
    "df_main.to_csv('python_02_main.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch the same file and add headers\n",
    "filepath = 'python_02_main.csv'\n",
    "column = ['complexity','token_count','loc','lloc','sloc','comments',\n",
    "          'multi','blank','code_comment','h1','h2','N1','N2','vocabulary','length',\n",
    "          'calculated_length','volume', 'difficulty','effort','time','bugs','change_type','added','removed',\n",
    "          'defect']\n",
    "df1 = pd.read_csv(filepath, usecols= column, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing using ordinal encoder\n",
    "categories =['ModificationType.ADD', 'ModificationType.DELETE', \n",
    "            'ModificationType.MODIFY','ModificationType.RENAME']\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "encoder = OrdinalEncoder()\n",
    "df1.change_type = encoder.fit_transform(df1.change_type.values.reshape(-1, 1))\n",
    "\n",
    "#Clean data by removing rows where LOC = 0, and complexity = 0\n",
    "col = ['loc','complexity']\n",
    "df1 = df1.replace(0, pd.np.nan).dropna(axis=0,how='any',subset=col).fillna(0).astype(int)\n",
    "\n",
    "#update data to CSV\n",
    "df1.to_csv('python_02.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
